### Title:  Mixture-of-Experts (MoE): The Birth and Rise of Conditional Computation
Institution: Rice University
Paper Link: https://cameronrwolfe.substack.com/p/conditional-computation-the-birth



### Title:  AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for Efficient MoE Inference
Institution: Peking University & Beijing Advanced Innovation Center for Integrated Circuits
Paper Link:https://arxiv.org/abs/2408.10284


Title: SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget
Conference: ACL 2024
Institution: Shanghai Jiao Tong University
Paper Link: https://aclanthology.org/2024.acl-long.363.pdf


Title: Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference
Conference: IPDPS 2024
Institution: The Ohio State University
Paper Link: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579139