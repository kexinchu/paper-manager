# Emerging memory technologies and their impacts on multi-modal applications

## Impacts on Multi-Model Applications
<img src="./pictures/multi-model-applications.jpg" width=600>

    - Faster cross-modal data retrieval(e.g., vision+text at once)
    - Easier for CPU, GPU, FPGA to share large multi-modal datasets

## Vector Search
- vector search
    - 基本流程：
        - 特征提取(Embedding): 通过BERT/CLIP等将原始数据转换为定长向量
        - 索引构建(Indexing): 为了快速查询，对vector集合建立索引(ANN - Approximate Nearest Neighbor算法)
        - 查询(Query): 
        - 相似度度量：欧氏距离，内积 或者 余弦相似度
    - ANN 算法
        - 暴力搜索 Brute-Force
        - 倒排 + PQ
        - 图结构：如HNSW（Hierarchical Navigable Small World）在图中导航快速找到近邻
        - 哈希方法(LSH)：将高维向量hash到 bucket 中，减少比较次数

## Papers

### Characterizing the Dilemma of Performance and Index Size in Billion-Scale Vector Search and Breaking It with Second-Tier Memory
```shell
Characterizing the Dilemma of Performance and Index Size in Billion-Scale Vector Search and Breaking It with Second-Tier Memory
- SJTU
- May 7 2024
```
- Key Point
    - 数据集 & 索引常存储在 SSD 上，Search受到I/O带宽限制 (the coarse-grained access of SSD **mismatches** the fine-grained random read required by vector indexes)
    - 为缓解I/O瓶颈，现有索引（图索引与聚类索引）典型做法是扩大索引规模：添加更多边或复制更多向量，以提升SSD带宽利用。但这会导致5.8–7.7×的数据膨胀（index amplification），增加存储成本
    - Motivation：RDMA/CXL 互连的远程 DRAM 或 NVM 等第二阶内存，兼具存储设备的容量与内存的细粒度访问（256 B 对比 SSD 的 4 KB），可天然契合向量索引的访问模式
    - Challenges：
        - second-tier memory access latency > local DRAM
        - 利用 RDMA/CXL等second-tier memory 细粒度的I/O优势
    - 针对图索引的优化
        - pipeline：跨查询间的异步I/O与计算并行
            - 发出内存读请求后，不等待其完成，立即切换至其他查询的distance计算
        - 细粒度存储：second-tier中不按照SSD的方式进行大块对齐和填充；改为使用CSR(Compressed Sparse Row)存储格式，减少存储碎片
    - 针对聚类索引的优化
        - 将向量与metadat分开存储，只在cluster中保存向量地址（8B），复制时仅复制地址，避免向量复制带来的开销
        - Though the decoupled layout can reduce the index size, it also decouples the original 0.1–2.2 MB I/O in to 60–68 × small random I/Os (100–384 B). 通过预处理做cluster-aware group 合并small node。回复读取的空间局部性

### CXL-ANNS
```shell
CXL-ANNS: Software-Hardware Collaborative Memory Disaggregation and Computation for Billion-Scale Approximate Nearest Neighbor Search
- ATC 2023
- KAIST
```
- Key Point
    - 传统方法要么压缩向量、降低准确度（compression approaches），要么将索引放在 SSD/PMEM 上，牺牲性能（hierarchical approaches），都无法同时兼顾高吞吐、低延迟和低存储放大
    - 利用CXL将DRAM串联成可拓展和组合的 second-tier memory pool + 使用CXL-ANNS(关系感知缓存，预取，parallel等技术) 提高性能。
    - Challenges:
        - 访问粒度与延迟不匹配：CXL access Lat > DRAM, 直接存储索引会拖慢Search
        - 大规模图遍历：ANNS 的图遍历从单一入口节点开始，随着跳数增加访问稀疏，如何识别热点节点并缓存，减少远内存访问？
        - 硬件协作设计：CXL RC 与 EP 如何协同分担计算与数据传输，最大化并行度？
        - 依赖性与调度：传统 ANNS 的 kNN 查询串行依赖严重，无法充分利用异构硬件并行能力，需重构执行依赖。
    - 核心设计：
        - 关系感知图缓存
            - ANNS 与靠近入口节点的图结构被访问的频率越高
                - 距离入口节点近的节点信息/vector等缓存在本地DRAM
                - 其他热点数据放置在CXL pool中
                - 动态 + 静态分析图访问频率，确定缓存集
            - Foreseeing Prefetch
                - 异步预取下一级节点数据到local DRAM,隐藏CXL Lat
                - 如何预测：利用图遍历的“最佳优先”特性
            - CXL 协同kNN search
            - 将kNN 查询拆分成 urgent + deferable 子任务，基于子任务优先级 + 可并行性来动态插入 deferable 子任务到计算空闲窗口

### 建议
- ANNS算法有哪些；根据CXL + DRAM的读写特性
- 算法的区别

### [FusionANNs](https://arxiv.org/html/2409.16576v1)
```shell
FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for Billion-scale Approximate Nearest Neighbor Search
```
- Key Point
    - ANNS 是 高内存 + 高计算密集型 application
        - 现有方法来降低ANNS所需的内存成本
            - Hierarchical Indexing (HI)
                - CPU memory, GPU HBM, SSD
            - Product Quantization (PQ)
        - GPU 被用来加速 ANNS 中的大量distance 计算。但是随着数据增加 + HBM容量优先，I/O成为主要bottleneck
        - ANNS系统面临的挑战：
            - 为了提高Search的准确性和效率，大多数ANNS系统利用复制策略构建高质量的 IVF 索引，其中边界向量被复制到相邻的发布列表中。这可以显著扩展索引的大小，8× memory开销
            - PQ 量化会导致精度损失
            - vector的大小通常在128 Byte, 但是NVMe SSD的最小处理单位是 4KB，导致显著的读取放大
                - Re-Ranking过程需要向SSD上发送大量I/O请求
                - re-ranking 与 ANNS的准确率相关
                <img src="./pictures/FusionANNS-accuracy-re-ranking.png" width=400>  
        - IVF Indexing
    - 本文解决的挑战
        - Hierarchical Indexing需要 cross 设备数据分布的设计来减少频繁的CPU - GPU 之间的数据传输
            - 将索引结构分层：
                - SSD存储 raw vectors
                - GPU-HBM 存储 PQ vectors
                - CPU memory 存储 graph indexing
        - 为了查询准确率，re-ranking 难以避免，但是re-ranking会造成大量I/O，需要在给定的准确率约束下，让每次查询都尽可能少执行re-ranking
            - Heuristic Re-ranking： 预设一个较大的re-ranking次数，但是检测到准确率满足后停止继续re-ranking
        - vector 细粒度 vs NVMe 4KB => 读放大
            - Redundant-aware I/O Deduplication: 识别并消除冗余的I/O请求

### [BANG](https://arxiv.org/html/2401.11324v4)
```
BANG: Billion-Scale Approximate Nearest Neighbour Search using a Single GPU
ArXiv 4/12/2025
```
- [Source-Code](https://github.com/karthik86248/BANG-Billion-Scale-ANN)
- Key Point
    - 结合数据压缩，CPU-GPU协同处理，内存管理，实现在单个GPU上处理十亿级ANNS的能力
    - ANNS 计算distance 计算密集型 => GPU加速
        - 基于图的ANNS算法 依赖读取graph index + vector data
        - PCIe 4.0 的峰值理论data transfor 速度 32GB/s (I/O bottleneck)
    - Graph Index
        - A graph-based ANNS algorithm runs on a proximity graph, which is pre-constructed over the dataset points by connecting each point to its nearby points. 
        - 选择效果最好的 the Vamana graph index (others like HNSW)
        - Graph Index的贪婪索引
            - 限制search中访问的节点数量, 降低search成本
            - 在图索引, 每个节点代表一个压缩后的向量,并链接到若干邻居. 由于无法一次性知道所有节点与查询向量之间的距离, 所以维护一个候选者列表, 从一个起始点开始 -> 逐步探索"有潜力"的节点 -> 计算查询向量 到 这些节点的距离 -> 循环搜索之后返回最优结果(满足终止条件)
    - 本文中
        - Graph Index存储在 CPU DRAM中, 由CPU来查询 起始点 和 潜力点 的neighbor nodes
        - GPU 批量计算 能查询vector 与 candidate nodes 的distance
        - candidate 队列 一般是 priority queue 或者 beam search (固定宽度堆) 
        - GPU Asymmetric Distance Computation
            - 每个数据库向量都使用 PQ（Product Quantization） 编码为多个小的子码（sub-code）。每个子码是原始向量某一子空间上的聚类索引值。
            - PQ 查表流程
                - 构建查找表（LUT）
                    - 查询向量被划分为 $M$ 个子向量。
                    - 每个子向量与该子空间的 codebook（聚类中心）计算欧氏距离。
                    - 对每个子空间 $m ∈ [1, M]$, 得到长度为K的表 $LUT_m [j] = ||q_m - c_{mj}||^2$; 其中 $c_{mj}$ 是第m个子空间的第j个中心.
                - 求distance
                    - 每个向量的编码变成序列 $code = [c_1,, c_2, ..., c_M]$
                    - 查询向量到该编码的距离:
                        $dist(q,x) = \sum_{m=1}^{M}{LUT_m[c_m]}$

    - shortage
        - 所有vector都需要经过PQ compression 然后存储在GPU HBM中
            - 十亿级vector, 原始需要 512GB (128-dim float), 可压缩到 ~1/16 甚至更低
            - 优点是无序PCIe频繁传输数据

### [HM-ANN](https://papers.nips.cc/paper/2020/hash/788d986905533aba051261497ecffcbb-Abstract.html?utm_source=chatgpt.com)
```shell
HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory
NeurIPS 2020
```
- Key Point
    - graph 结构
        - 上层（如 L1 及以上）：包含数据子集，存储在 DRAM 中，便于快速访问。
        - 底层（L0）：包含完整数据集，存储在 PMem 中，访问速度较慢。
        - 通过将大部分search ops限制在DRAM中的上层，减少对PMem的访问，提升查询效率
    - 查询算法优化：
        - 快速内存搜索 - 从最顶层开始，逐层进行贪婪搜索，主要在DRAM中完成
        - 并行底层搜索 + 预取
            - 使用来自 L1 的多个候选节点作为入口点，在 L0 层并行执行多线程搜索。
            - 在搜索 L1 时，预取相关的 L0 数据到 DRAM 中的缓冲区，减少后续访问 PMem 的延迟。

### [ESPN](https://arxiv.org/pdf/2312.05417)
```shell
ESPN: Storage Access Optimization for Efficient GPU-centric Information Retrieval
March 2025
```
- Key Point
    - 利用NVIDIA的GPUDirect Storage技术，实现了数据从SSD直接传输到GPU内存，绕过了传统的CPU主导的数据传输路径，从而减少了延迟并提高了吞吐量。
    - embedding vectors 存储在SSD上；candidate index 和 metadata(IVF/HNSW索引结构) 存储在 DRAM 中
    - 软件预取
        - 根据ANN candidate 的分数或者distance进行排序，选择topk embeddings 提前加载到GPU
        - 异步处理
    - GPU上的早期重排序
        - 一旦embedding 被预取到GPU，利用GPU高吞吐的矩阵乘法能力计算与查询向量的相似度
        - 对candidates进行early re-ranking, 进一步筛选出 top-k中的最优结果
        - 避免为低质量 candidate 加载不必要的embedding


## Idea

- CPU - GPU 协同搜索
    - 基于图的 ANN 搜索主要涉及两个操作：图导航（跟踪指针、检查已访问标记、选择下一个邻居）和距离计算（计算高维向量之间的相似性）。CPU 擅长前者（不规则的内存访问和控制密集型逻辑），而 GPU 则擅长后者（并行处理大量数据的密集算法）。
    - Bang 将全部graph index 存入 DRAM + 全部PQ之后的vector存入 HBM 加速
        - 分阶段pipeline：GPU计算一批neighbor node的distance，排序，结果合并的同时，CPU获取下一批candidate nodes 并传输给GPU
        - 缺点：1，PQ会降低准确度； 2，DRAM/HBM容量有限； 3，GPU/CPU负载diff，导致有时需要多个CPU才能满足GPU的高效处理能力 (CPU争用)

    - FusionANNS GPU - CPU - SSD hierarchical memory
        - 通过多层索引；使用CPU对candidate进行初步筛选来降低需要传输给 GPU 的数据量
        - 缺点：需要确保 CPU 能够足够快地为邻近处理器提供数据，以保持 GPU 的繁忙至关重要。如果 CPU 遍历速度太慢或 GPU 速度太快，就会有一个处理器空闲。
    
    - 思路：
        - 考虑到 HBM/DRAM的 有限容量；采用SSD来存储 graph index(metadata)/vector 数据
        - 借助 GPUDirect RDMA / CXL 实现 CPU - CXL 和 GPU - CXL 的互联，避免CPU征用的同时提供更大的容量

- 基于SSD存储的分布式架构
    - SPANN（NeurIPS'21）仅将聚类中心和顶层图存储在内存中，而将完整向量（倒排列表）存储在 SSD 上。
    - ESPN 将vector完全卸载到SSD上，并使用GPUDirect 预取来优化性能 (绕过CPU)
        - 缺点： SSD的I/O容易成为瓶颈

    - 思路：
        - 在分布式 multi node的场景下，如何对 graph 分区？
        - 不均匀的分区会导致负载不均衡，同时造成过多的跨node neighbor search开销
        - 部分复制可以一定程度上减少cross node的search，但是会额外消耗存储
        - 思路： 
            - 借助 CXL share memory 来存储graph index，避免cross node search
            - vector数据上，借助 CXL - SSD结构 (通过candidate 来预取vector 到HBM 和 CXL)
            - hot 数据需要尽可能保留再local DRAM 或者 CXL上 vs cold
            - prefetch 的时机
                - 过于激进，会导致预取错误/无效数据，浪费带宽 和 缓存空间
                - 预取太晚，CPU/GPU waiting for data， 造成graph search的慢

- Multi-tier Memory with CXL
    - CXL-ANNS 利用CXL将DRAM串联成可拓展和组合的 second-tier memory pool；将所有数据存储CXL Mem中，然后在CPU侧 local DRAM 上做缓存
    - 接近入口的图结果存储在local DRAM中 + prefetch
    - 挑战：
        - 全部存储在CXL memory中依赖 DRAM容量
        - 如果多个节点和GPU都哦访问同一个CXL memory pool，CXL链路或者内存设备可能需要发生争用
            - 对memory pool进行分区
            - 使用多个扩展设备

- 汇总
    - 背景&出发点：
        - multi-modal applications 中依赖多种数据(文本，语音，视频，图片等)，请从这里切入，转入vector存储和ANNS (说明多模态 和 vector存储之间的关系)
        - 考虑到vector数据量的增加，以及CPU适合计算graph search，GPU适合计算distance的特点。说明设计一个分布式 ANNS system的必要性，实现快速 and 低成本的ANNS
        - 传统方法的说明：1，Bang考虑使用GPU 和 CPU特性来分别计算不同部分，但是要求将PQ之后的vector存入HBM + graph index存入local DRAM；随着数据增加，需要更多资源，甚至无法使用。2，分布式方案(列出几篇论文) 借助 SSD - CPU DRAM的multi-tier存储，并采用pipeline + prefetch方法(有哪些，要列出来)，但是也有其缺点：graph等数据在每个node上都需要存储 -> 冗余存储, IO + SSD访问速度； cross node access， 以及CPU争用问题；3，借助CXL方法的 (CXL-ANNS) 作为比SSD更快的二级存储，可以取得比SSD更快的性能，但是依赖再CXL memory中存储全部数据
        - 结合上面的这些工作和问题，我们提出我们的设计：下面的 multi-tier memory设计； 可以避免数据争用(CPU/GPU 独立的与CXL交互)
    - 核心： multi-tier memory
        - tier 0: GPU HBM; CPU local DRAM  作为local Cache来提供尽可能快读取，满足CPU/GPU计算的要求 (CPU计算grgph， GPU计算 distance,re-ranking，等)
        - tier 2: CXL memory pool  存储graph 和 vectors 中的hot数据 (容量可能依旧不够)
        - tier 3: SSD 持久化存储全部的graph，vector，raw data 数据
    
    - 挑战
        - CXL memory的 read latency > local DRAM；
        - 选择合适的时机开始prefetch？
        - 充分利用 有限容量的local DRAM/HBM 和 CXL memory，减少cache miss
    
<!-- - Solutions： -->
<!-- - 通过pipeline的方式隐藏数据传输开销
    - CPU计算neighbor node，异步传输candidate给GPU； gpu异步传输re-rank的topk给CPU
    - 根据candidate prefetch vectors to GPU HBM
    - 根据topk prefetch sub-graph to CPU DRAM
    - asynchronous data transfor
- CPU sub graph的prefetch， GPU vector 数据的prefetch 以及 
- hot/cold数据区分 + CXL - DRAM数据交互 -->

- 强调multi-model
    - 问题：
        - 问题1，强调跨模态检索；混合模态检索虽然处理起来更容易，更理想。但是可能会产生模态同质的聚类或图邻域（例如，图像主要位于图像附近，文本位于文本附近），这可能会影响基于图的索引导航：例如 一个图像节点的最近邻可能是一个描述它的文本节点，但它的大多数邻居都是其他图像
        - 问题2，针对不同的查询类型进行优化：纯文本查询可能只需要搜索文本向量，而针对图像的图像查询则可能完全忽略文本向量。设计一个能够根据模态动态限制搜索空间的系统（例如使用特定于模态的路由过滤器）将节省计算资源。
        - 问题3，多模态索引往往更大（每个项目可能贡献多个embedding）；仅依赖GPU HBM 和 CPU DRAM 不足以存储vectors 和 graph indexing信息；使用PQ的方法则会损失精度和准确度
            - 传统的方法使用SSD来存储 vectors 和 graph index信息。但是SSD基于page的读写模式(如4KB) 和 细粒度vector(128*8Byte)的mismatch 会导致read amplification

    - 核心不变，还是借助 GPU/CPU - CXL 来管理graph index 和 vetors
        - 利用CXL来存储graph index 和 vector信息，提供额外存储空间的同时解决 SSD 的read amplification 问题
        - CPU 和 GPU 独立的与CXL进行数据transfer，避免对CPU资源和CPU带宽造成争用
    
    - 使用 GPU/CPU - CXL 带来的新挑战
        - CXL 的 bandwidth 较低；可能成为系统的瓶颈
        - CXL 的 latency 要高于 CPU 访问 local DRAM 或者 GPU访问 HBM, 即使在使用pipeline + asnyc data transfer，仍会导致GPU/CPU 等待数据(最好找一些论证)
        - 传统的ANNS主要针对单一模态或者混合模态数据，并不能很好解决多模态应用中的问题

    - solutions：
        - 解决挑战1： pipeline + asynchro transfer 来hide bandwidth问题
            - CPU进行 query 的 graph search； GPU计算vector的distance，倒排等信息
            - CPU计算完 query A 的graph search，异步传输candidate给GPU，GPU接收之后开始异步读取所需的vectors； 当GPU完成当前正在计算的query B的计算，立刻开始计算 query A。同时异步将re-rank得到topk传给CPU 计算下一步的graph search。实现一个pipeline，优先保障GPU的计算utilization
        - 解决挑战2：prefetch
            - CPU计算过程中，逐步获取candidate nodes(可能不是最终的，需要根据ANNS算法来判断)，同根据这些candidate prefetch vectors to GPU HBM
            - 同时GPU vector + rerank的时候，根据topM (M可以仅仅是top2/3/4， M << k) prefetch sub-graph to CPU DRAM
            - 这里需要阅读一些相关论文，丰富设计 和 细节
        - 解决挑战3：针对多模态的 模态感知neighbor选择
            - 引入模态权重策略，在图构建 和 查询阶段加入模态信息
            - 图构建阶段
                - vector 附加模态标签，对于每个node
                    - 保留一部分跨模态的“语义邻居”，即使它们距离略远。
                    - 引入跨模态强连边（modality bridge edges）
                - 类似图正则化或补边策略，提升模态间可达性。
            - 查询阶段
                - query 计算vector时包含模态信息
                - 选择neighbor时
                    - 借鉴启发式搜索 heuristic search
                    - 如果当前candidate node模态与目标模态不符，在优先队列中对其邻居进行“模态跳跃”加权提升；
                    - 实施一个简单的模态惩罚因子：
                        $adjusted_score = distance + \lambda * loss_{modaility_mismatch}$
