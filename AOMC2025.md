# Emerging memory technologies and their impacts on multi-modal applications

## Impacts on Multi-Model Applications
<img src="./pictures/multi-model-applications.jpg" width=600>

    - Faster cross-modal data retrieval(e.g., vision+text at once)
    - Easier for CPU, GPU, FPGA to share large multi-modal datasets

## Vector Search
- vector search
    - 基本流程：
        - 特征提取(Embedding): 通过BERT/CLIP等将原始数据转换为定长向量
        - 索引构建(Indexing): 为了快速查询，对vector集合建立索引(ANN - Approximate Nearest Neighbor算法)
        - 查询(Query): 
        - 相似度度量：欧氏距离，内积 或者 余弦相似度
    - ANN 算法
        - 暴力搜索 Brute-Force
        - 倒排 + PQ
        - 图结构：如HNSW（Hierarchical Navigable Small World）在图中导航快速找到近邻
        - 哈希方法(LSH)：将高维向量hash到 bucket 中，减少比较次数

## Papers

### Characterizing the Dilemma of Performance and Index Size in Billion-Scale Vector Search and Breaking It with Second-Tier Memory
```shell
Characterizing the Dilemma of Performance and Index Size in Billion-Scale Vector Search and Breaking It with Second-Tier Memory
- SJTU
- May 7 2024
```
- Key Point
    - 数据集 & 索引常存储在 SSD 上，Search受到I/O带宽限制 (the coarse-grained access of SSD **mismatches** the fine-grained random read required by vector indexes)
    - 为缓解I/O瓶颈，现有索引（图索引与聚类索引）典型做法是扩大索引规模：添加更多边或复制更多向量，以提升SSD带宽利用。但这会导致5.8–7.7×的数据膨胀（index amplification），增加存储成本
    - Motivation：RDMA/CXL 互连的远程 DRAM 或 NVM 等第二阶内存，兼具存储设备的容量与内存的细粒度访问（256 B 对比 SSD 的 4 KB），可天然契合向量索引的访问模式
    - Challenges：
        - second-tier memory access latency > local DRAM
        - 利用 RDMA/CXL等second-tier memory 细粒度的I/O优势
    - 针对图索引的优化
        - pipeline：跨查询间的异步I/O与计算并行
            - 发出内存读请求后，不等待其完成，立即切换至其他查询的distance计算
        - 细粒度存储：second-tier中不按照SSD的方式进行大块对齐和填充；改为使用CSR(Compressed Sparse Row)存储格式，减少存储碎片
    - 针对聚类索引的优化
        - 将向量与metadat分开存储，只在cluster中保存向量地址（8B），复制时仅复制地址，避免向量复制带来的开销
        - Though the decoupled layout can reduce the index size, it also decouples the original 0.1–2.2 MB I/O in to 60–68 × small random I/Os (100–384 B). 通过预处理做cluster-aware group 合并small node。回复读取的空间局部性

### CXL-ANNS
```shell
CXL-ANNS: Software-Hardware Collaborative Memory Disaggregation and Computation for Billion-Scale Approximate Nearest Neighbor Search
- ATC 2023
- KAIST
```
- Key Point
    - 传统方法要么压缩向量、降低准确度（compression approaches），要么将索引放在 SSD/PMEM 上，牺牲性能（hierarchical approaches），都无法同时兼顾高吞吐、低延迟和低存储放大
    - 利用CXL将DRAM串联成可拓展和组合的 second-tier memory pool + 使用CXL-ANNS(关系感知缓存，预取，parallel等技术) 提高性能。
    - Challenges:
        - 访问粒度与延迟不匹配：CXL access Lat > DRAM, 直接存储索引会拖慢Search
        - 大规模图遍历：ANNS 的图遍历从单一入口节点开始，随着跳数增加访问稀疏，如何识别热点节点并缓存，减少远内存访问？
        - 硬件协作设计：CXL RC 与 EP 如何协同分担计算与数据传输，最大化并行度？
        - 依赖性与调度：传统 ANNS 的 kNN 查询串行依赖严重，无法充分利用异构硬件并行能力，需重构执行依赖。
    - 核心设计：
        - 关系感知图缓存
            - ANNS 与靠近入口节点的图结构被访问的频率越高
                - 距离入口节点近的节点信息/vector等缓存在本地DRAM
                - 其他热点数据放置在CXL pool中
                - 动态 + 静态分析图访问频率，确定缓存集
            - Foreseeing Prefetch
                - 异步预取下一级节点数据到local DRAM,隐藏CXL Lat
                - 如何预测：利用图遍历的“最佳优先”特性
            - CXL 协同kNN search
            - 将kNN 查询拆分成 urgent + deferable 子任务，基于子任务优先级 + 可并行性来动态插入 deferable 子任务到计算空闲窗口

### 建议
- ANNS算法有哪些；根据CXL + DRAM的读写特性
- 算法的区别

### [FusionANNs](https://arxiv.org/html/2409.16576v1)
```shell
FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for Billion-scale Approximate Nearest Neighbor Search
```
- Key Point
    - ANNS 是 高内存 + 高计算密集型 application
        - 现有方法来降低ANNS所需的内存成本
            - Hierarchical Indexing (HI)
                - CPU memory, GPU HBM, SSD
            - Product Quantization (PQ)
        - GPU 被用来加速 ANNS 中的大量distance 计算。但是随着数据增加 + HBM容量优先，I/O成为主要bottleneck
        - ANNS系统面临的挑战：
            - 为了提高Search的准确性和效率，大多数ANNS系统利用复制策略构建高质量的 IVF 索引，其中边界向量被复制到相邻的发布列表中。这可以显著扩展索引的大小，8× memory开销
            - PQ 量化会导致精度损失
            - vector的大小通常在128 Byte, 但是NVMe SSD的最小处理单位是 4KB，导致显著的读取放大
                - Re-Ranking过程需要向SSD上发送大量I/O请求
                - re-ranking 与 ANNS的准确率相关
                <img src="./pictures/FusionANNS-accuracy-re-ranking.png" width=400>  
        - IVF Indexing
    - 本文解决的挑战
        - Hierarchical Indexing需要 cross 设备数据分布的设计来减少频繁的CPU - GPU 之间的数据传输
            - 将索引结构分层：
                - SSD存储 raw vectors
                - GPU-HBM 存储 PQ vectors
                - CPU memory 存储 graph indexing
        - 为了查询准确率，re-ranking 难以避免，但是re-ranking会造成大量I/O，需要在给定的准确率约束下，让每次查询都尽可能少执行re-ranking
            - Heuristic Re-ranking： 预设一个较大的re-ranking次数，但是检测到准确率满足后停止继续re-ranking
        - vector 细粒度 vs NVMe 4KB => 读放大
            - Redundant-aware I/O Deduplication: 识别并消除冗余的I/O请求

### [BANG](https://arxiv.org/html/2401.11324v4)
```
BANG: Billion-Scale Approximate Nearest Neighbour Search using a Single GPU
ArXiv 4/12/2025
```
- [Source-Code](https://github.com/karthik86248/BANG-Billion-Scale-ANN)
- Key Point
    - 结合数据压缩，CPU-GPU协同处理，内存管理，实现在单个GPU上处理十亿级ANNS的能力
    - ANNS 计算distance 计算密集型 => GPU加速
        - 基于图的ANNS算法 依赖读取graph index + vector data
        - PCIe 4.0 的峰值理论data transfor 速度 32GB/s (I/O bottleneck)
    - Graph Index
        - A graph-based ANNS algorithm runs on a proximity graph, which is pre-constructed over the dataset points by connecting each point to its nearby points. 
        - 选择效果最好的 the Vamana graph index (others like HNSW)
        - Graph Index的贪婪索引
            - 限制search中访问的节点数量, 降低search成本
            - 在图索引, 每个节点代表一个压缩后的向量,并链接到若干邻居. 由于无法一次性知道所有节点与查询向量之间的距离, 所以维护一个候选者列表, 从一个起始点开始 -> 逐步探索"有潜力"的节点 -> 计算查询向量 到 这些节点的距离 -> 循环搜索之后返回最优结果(满足终止条件)
    - 本文中
        - Graph Index存储在 CPU DRAM中, 由CPU来查询 起始点 和 潜力点 的neighbor nodes
        - GPU 批量计算 能查询vector 与 candidate nodes 的distance
        - candidate 队列 一般是 priority queue 或者 beam search (固定宽度堆) 
        - GPU Asymmetric Distance Computation
            - 每个数据库向量都使用 PQ（Product Quantization） 编码为多个小的子码（sub-code）。每个子码是原始向量某一子空间上的聚类索引值。
            - PQ 查表流程
                - 构建查找表（LUT）
                    - 查询向量被划分为 $M$ 个子向量。
                    - 每个子向量与该子空间的 codebook（聚类中心）计算欧氏距离。
                    - 对每个子空间 $m ∈ [1, M]$, 得到长度为K的表 $LUT_m [j] = ||q_m - c_{mj}||^2$; 其中 $c_{mj}$ 是第m个子空间的第j个中心.
                - 求distance
                    - 每个向量的编码变成序列 $code = [c_1,, c_2, ..., c_M]$
                    - 查询向量到该编码的距离:
                        $dist(q,x) = \sum_{m=1}^{M}{LUT_m[c_m]}$

    - shortage
        - 所有vector都需要经过PQ compression 然后存储在GPU HBM中
            - 十亿级vector, 原始需要 512GB (128-dim float), 可压缩到 ~1/16 甚至更低
            - 优点是无序PCIe频繁传输数据

### [HM-ANN](https://papers.nips.cc/paper/2020/hash/788d986905533aba051261497ecffcbb-Abstract.html?utm_source=chatgpt.com)
```shell
HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory
NeurIPS 2020
```
- Key Point
    - graph 结构
        - 上层（如 L1 及以上）：包含数据子集，存储在 DRAM 中，便于快速访问。
        - 底层（L0）：包含完整数据集，存储在 PMem 中，访问速度较慢。
        - 通过将大部分search ops限制在DRAM中的上层，减少对PMem的访问，提升查询效率
    - 查询算法优化：
        - 快速内存搜索 - 从最顶层开始，逐层进行贪婪搜索，主要在DRAM中完成
        - 并行底层搜索 + 预取
            - 使用来自 L1 的多个候选节点作为入口点，在 L0 层并行执行多线程搜索。
            - 在搜索 L1 时，预取相关的 L0 数据到 DRAM 中的缓冲区，减少后续访问 PMem 的延迟。

### [ESPN](https://arxiv.org/pdf/2312.05417)
```shell
ESPN: Storage Access Optimization for Efficient GPU-centric Information Retrieval
March 2025
```
- Key Point
    - 利用NVIDIA的GPUDirect Storage技术，实现了数据从SSD直接传输到GPU内存，绕过了传统的CPU主导的数据传输路径，从而减少了延迟并提高了吞吐量。
    - embedding vectors 存储在SSD上；candidate index 和 metadata(IVF/HNSW索引结构) 存储在 DRAM 中
    - 软件预取
        - 根据ANN candidate 的分数或者distance进行排序，选择topk embeddings 提前加载到GPU
        - 异步处理
    - GPU上的早期重排序
        - 一旦embedding 被预取到GPU，利用GPU高吞吐的矩阵乘法能力计算与查询向量的相似度
        - 对candidates进行early re-ranking, 进一步筛选出 top-k中的最优结果
        - 避免为低质量 candidate 加载不必要的embedding

###[DiskANN](https://papers.nips.cc/paper_files/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf)
```shell
Title: DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node
Conference: NeurIPS 2019
```
- Key Point
    - 

## Idea
<!-- - 背景&现有工作：
    - 考虑到vector数据量的增加，以及CPU适合计算graph search，GPU适合计算distance的特点。说明设计一个分布式 ANNS system的必要性，实现快速 and 低成本的ANNS
    - 传统方法的说明：
        - 1，Bang考虑使用GPU 和 CPU特性来分别计算不同部分，但是要求将PQ之后的vector存入HBM + graph index存入local DRAM；随着数据增加，需要更多资源，甚至无法使用。
        - 2，分布式方案借助 SSD - CPU DRAM的multi-tier存储，并采用pipeline + prefetch方法，但是也有其缺点：graph等数据在每个node上都需要存储 -> 冗余存储, IO + SSD访问速度； cross node access， 以及CPU争用问题；
        3，借助CXL方法的 (CXL-ANNS) 作为比SSD更快的二级存储，可以取得比SSD更快的性能，但是依赖在CXL memory中存储全部数据 (CXL-ANNS将图索引缓存在DRAM中，CXL只存储cold data)
    - 结合上面的这些工作和问题，我们提出我们的设计：下面的 multi-tier memory设计； 可以避免数据争用(CPU/GPU 独立的与CXL交互) -->

- 解决什么问题？
    - 我们聚焦在 billion-scale 多模态 ANN 搜索中，由以下三类结构性挑战带来的系统与算法协同瓶颈：
    - 模态同质性聚类，阻碍跨模态搜索
        - 在多模态嵌入空间中，图像、文本等模态往往形成结构紧密的子簇（clumps），如图像邻居大多还是图像。
        - ANNS 算法如 HNSW、NSG 的搜索过程是基于图+greedy算法进行贪心+局部跳转的，一旦落入某一模态子图，很容易“陷入局部”而无法跨模态发现更优匹配。
        - 解决方法需扩大搜索半径或强行跨模态跳跃，这会显著增加访问向量数量与图节点，尤其在 billion-scale 场景中压力极大。
    - 查询模态不一致导致不必要的扩展
        - 用户的查询行为可能偏向特定模态，例如“只查图像”或“文本问图像”。
        - 若搜索过程模态无感知，会在无关模态中做大量冗余扩展、距离计算、排序，浪费带宽与计算。
    - 多模态数据带来爆炸式索引量 + 存储放大问题
        - 每个数据项可能生成多个模态嵌入，而存储全部graph索引与向量超出HBM/DRAM能力。
        - 传统向量压缩(PQ)虽可节省空间，但在多模态任务中会降低结果精度；而且随着数据的进一步增加，压缩方案也无法满足HBM+DRAM
        - 传统方案依赖SSD，但由于小向量与大页粒度间的mismatch，严重read amplification影响性能。(放大率5-8X)

- Motivation
    - 结合上面的多模态 "局部结果" -> 需要在HNSW算法中使用更大的 neighbors 和 beam search宽度 -> 访问更多的子图(随机访问) + vectors
    - SSD的access放大特性使其无法满足 多模态场景下的ANNS search方法；而且ANNS: GPU + CPU 的workload场景，使用SSD，vector读取依赖CPU处理；
    - CXL-Memory的"大容量，低放大，并行访问，CPU/GPU直接访问"特性使其成为更好的选择
    - Note: CXL在多模态嵌入空间的特殊贡献
    - 不同模态数据 的size不同，考虑search之后access的数据传输 => scheduler的优化(考虑SLO)

    - 核心思路：multi-tier memory
        - tier 1: GPU HBM; CPU local DRAM  作为local Cache来提供尽可能快读取，满足CPU/GPU计算的要求 (CPU计算grgph， GPU计算 distance,re-ranking，等)
        - tier 2: 利用CXL来存储graph index 和 vector信息，提供额外存储空间的同时解决 SSD 的read amplification 问题
        - CPU 和 GPU 独立的与CXL进行数据transfer，避免对CPU资源和CPU带宽造成争用

- 新设计面临的挑战：
    - CXL的关键特性
        - CXL是一种基于PCIe的高速内存互联标准；其latency(~150-300ns) > local DRAM; 带宽受限于PCIe Channel
        - CXL.mem 可以挂在大容量DRAM,支持内存扩展
        - 缓存一致性(CXL.cache)； CPU/GPU可以通过load/store方式share CXL memory
    - 挑战：
        - 高延迟 + 随机访问不匹配
            - ANNS中，Graph traversal（pointer chasing）和向量 fetch 都是高度随机访问操作，本地 DRAM 可快速应对，但 CXL 的随机访问延迟会显著降低性能
            - 而且在 多模态下，更大的beam search搜索半径，会导致更多的随机访问
        - 多核 CPU/GPU同时请求CXL，容易触发PCIe带宽冲突
        - 模态感知的ANNS算法
            - 现有的ANNS算法并不支持模态感知，导致并不能很好解决多模态应用中的问题(只能从全局检索，并不能只检索目标模态的数据)
        - CXL(底层设备)以4KB为访问的单元，导致read amplification => 将多个vector读取合并程一个batch request, 借助DMA拉去

- solutions：
    - M-CANS设计 Overview
        - CPU 仅处理图（Graph）数据，并将其从 CXL 预取至 CPU DRAM；
        - GPU 仅处理向量（Vector）数据，并将其直接从 CXL 预取至 GPU HBM；
        - CPU-CXL, GPU-CXL 的数据通路 完全解耦，避免冗余缓存和带宽冲突。
    
    - 异步预取 + 缓存
        - 使用CPU DRAM缓存热点子图 (维护一个subgraphs cache)
            - 使用LRU策略管理cache，支持批量evict old subgraphs
        - Graph prefetching to DRAM
            - 在beam search中，if topk 的多个候选节点位于同一个子图中，预取子图
        - Vector prefetching to HBM
            - GPU 直接定向从 CXL‑HBM 异步预取，在 CUDA pipeline 中与 distance compute 重叠。
            - 预测下一批vectors

    - 带宽感知scheduler
        - 结合当前CPU/GPU利用率，分别调度graph 和 vector的带宽占比
        - 借助graph task queue + vector task queue来充分利用CXL异步带宽
    
    
    - 解决挑战3：模态感知ANNS算法
        - 调整：有一个 target 模态，如何在search中跳过不同模态的簇
        - 图构建阶段
            - vector 附加模态标签，对于每个node
                - 保留一部分跨模态的“语义邻居”，即使它们距离略远。
                - 引入跨模态强连边（modality bridge edges）
            - 类似图正则化或补边策略，提升模态间可达性。
            - Note: 创建时增加一些随机性 [快速跑一下]
        - 查询阶段
            - query 计算vector时包含模态信息
            - 选择neighbor时
                - 借鉴启发式搜索 heuristic search
                - 如果当前candidate node模态与目标模态不符，在优先队列中对其邻居进行“模态跳跃”加权提升；
                - 实施一个简单的模态惩罚因子：
                    $adjusted_score = distance + \lambda * loss_{modaility_mismatch}$

    - vector/graph批量聚合读取
        - Vector Path：将一批向量打包为一个大块 DMA 读取，减少 random read 开销。
        - Graph Path：将预取子图 group by cluster，一次性读取多个节点邻接信息。

- Motivation Test
    - SSD的读写放大问题 in ANNS
    - 当前 ANNS 算法在multi-model中不适应的证明

- Note
    - CXL 关联多个host
    - cache coherient；node A 写 -> node B 感知到

- Benchmark
    - [ANN-Benchmarks](https://github.com/erikbern/ann-benchmarks/)
    - [多模态数据集](https://learning2hash.github.io/tags.html#Dataset)
        - [Yandex Text-to-Image-1B](https://research.yandex.com/blog/benchmarks-for-billion-scale-similarity-search)
            - Image Embedding (dim=200; model=Se-ResNeXt-101)
            - Text Embedding (dim=200; model=DSSM)
        - [BigANN](https://big-ann-benchmarks.com/neurips23.html)
        - [Qilin](https://arxiv.org/pdf/2503.00501?)