### Title: Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU 
Conference: arXiv July 8 2024    
Institution: Peking University    
Paper Link: https://arxiv.org/pdf/2407.05858    
Source code: https://github.com/UbiquitousLearning/mllm   

##### Key Point
- on-device LLMs still suffer from unacceptably long inference latency, especially the TTFT
    - Gemma-2B need 26.7s to reply email
- use on-device NPU (mobile phone)
- system-algorithm co-design



