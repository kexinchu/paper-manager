# Chain of Thought
- 2024-9-13, OpenAI 发布了新的o1推理模型，在性能上超越了GPT-4o. 并首次在语言模型领域实现了强化学习和“私密思维链”

### Title: Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
Institution: Stanford & Google
Conference: ICLR 2024
Paper Link: https://arxiv.org/pdf/2402.12875
Source Code: 

### Title: Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters 
Institution: UC Berkeley & DeepMind
Conference: 
Paper Link:
Source Code:

##### Key Point
- 关键假设出发点：LLMs通过使用more test-time computation 可以改进其输出
- 本文在 Math 问题上，比较了不同难度问题上，不同实现参数配置下，两种test-time compute机制的评测结果
    - searching against a verifier/output level
        - process-based verifier reward models as step scorer + step/process search methods(step level sampling: Best of N, beam search, look-ahead)
        - 即使用基于过程的奖励模型（PRM）验证答案中各个步骤的正确性
    - refining the proposal distribution/input token level
        - updating the model's distribution over a response adaptively, given the prompt at test time;
        - **finetune models to iteratively revise their answers.**

### Title: Large Language Monkeys: Scaling Inference Compute with Repeated Sampling
Institution:
Conference:
Paper Link:
Source Code:

##### Key Point 
